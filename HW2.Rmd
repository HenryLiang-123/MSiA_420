---
title: "HW2"
output: pdf_document
date: "2023-02-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(boot)
library(caret)
library(projpred)
library(MASS)
library(car)
library(nnet)
library(rpart)
library(tidyverse)
```

## Question 1

### a

```{r q1a, echo=FALSE}
########### QUESTION 1 begins here ################### 
dat <- read_excel("HW2_data.xls")
dat <- dat[2:10]
dat$cost <- log10(dat$cost)

dat_scaled <- as.data.frame(cbind(dat$cost, scale(dat[2:9])))
names(dat_scaled) <- names(dat)

m1 <- lm(cost ~., data = dat_scaled)
vif(m1)
summary(m1)
```

As seen from the above model output, the R-squared is 0.5831, which means that out model is able to explain 58% of the variation in the data. The model seems to fit fairly well to the data, but some predictors are insignificant.

### b

As seen from the above model output, dur, the number of comorbidities, and the number of interventions or procedures carried out are the top 3 variables with the highest influence on cost as their coefficients are the largest. Note the VIF output for each variable is close to 1, meaning little to no multicolinearity. Also note from the normal QQ plot below that our residuals are normally distributed. This makes our estimates of the regression coefficients reliable

### c

```{r q1c, echo=FALSE}
par(mfrow = c(2, 2))
plot(m1)
```

As seen from the diagnostic plots above, the residual vs fitted plot shows that the residuals have non-linear patterns, which means that the linear model does not explain the non-linear relationships that was in the data, which is expected. Looking at the scale-location plot, we can see that there is a clear trend, which means that we have are violating the assumption of equal variance of the residuals for every level of independent variables.

## Question 2

### a

```{r q2a, echo=FALSE}
########### QUESTION 2 begins here ################### 
set.seed(123)
Nrep <- 3 #number of replicates of CV
K <- 10  #K-fold CV on each replicate
n.models <- 12 #number of different models to fit
size <- c(1,2,3)
decay <- c(0.001, 0.01, 0.1, 0)
n <- nrow(dat_scaled)
y <- dat_scaled$cost
yhat <- matrix(0, n, n.models)
MSE <- matrix(0, Nrep, n.models)
output <- data.frame()
model_num <- 1
for (s in size) {
  for (d in decay) {
    for (j in 1:Nrep) {
      cv_idx <- createFolds(dat_scaled$cost, k = K)
      for (k in 1:K) {
        out <-
          nnet(
            cost ~ .,
            data = dat_scaled[-cv_idx[[k]],],
            linout = T,
            skip = F,
            size = s,
            decay = d,
            maxit = 1000,
            trace = F
          )
         yhat[cv_idx[[k]], model_num] <- as.numeric(predict(out, dat_scaled[cv_idx[[k]], ]))
      } #end of k loop
      MSE[j, ] <- apply(yhat, 2, function(x)
        sum((y - x) ^ 2)) / n
    }#end of j loop
    output <- rbind(output, c(s, d))
    model_num <- model_num + 1
  }
}
```

```{r q2a_cont, echo=FALSE}
MSEAve <- apply(MSE,2,mean)
output <- cbind(output, MSEAve)
names(output) <- c("size", "decay", "Average CV Error over 3 replicates")
output
```

### b

```{r q2b, echo=FALSE}
set.seed(123)
nn1 <- nnet(cost ~ ., data = dat_scaled, linout=T, skip=F, size=2, decay=0.001, maxit=1000, trace=F)
yhat <- as.numeric(predict(nn1))
y <- dat_scaled$cost
mse <- mean((y - yhat)^2)
r2 <- 1 - mse/var(y); r2 
```

As seen above, with the size = 2 and decay = 0.001 from CV, the neural network is able to explain `r r2 * 100` % of the variance.

### c

```{r q2c, echo=FALSE}
library(ALEPlot)
yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata))
nn1 <- nnet(cost ~ ., data = dat_scaled, linout=T, skip=F, size=2, decay=0.001, maxit=1000, trace=F)
par(mfrow=c(2,4))
for (j in 1:8) {
  ALEPlot(data.frame(dat_scaled[, 2:9]), nn1, pred.fun=yhat, J=j, K=50, NA.plot = TRUE)
  }
 ## This creates main effect ALE plots for all 8 predictors
```

As seen from the ALE plots, ervis, comp, and comorb, and intvn have the most significant effect on cost. They all have a positive relationship with cost.

### d

```{r q2d, echo=FALSE}
pred <- predict(nn1, dat_scaled)
res <- dat_scaled$cost - pred
plot(pred, nn1$residuals, main = "Fitted vs Residuals")
```

AS seen from the plot above, all non-linear relationships have been captured.

## Question 3

### a

```{r q3a, echo=FALSE}
########### QUESTION 3 begins here ################### 
set.seed(55)
control <- rpart.control(minbucket = 5, cp = 0.01)
out <- rpart(cost ~ ., data = dat_scaled, method = "anova", control = control)
plotcp(out)
control_opti <- rpart.control(minbucket = 5, cp = 0.017)
out_opti <- rpart(cost ~ ., data = dat_scaled, method = "anova", control = control)
Nrep <- 3 #number of replicates of CV
K <- 10  #K-fold CV on each replicate
n.models <- 1 #number of different models to fit
cp <- c(0.001,0.01,0.1,0)
n <- nrow(dat_scaled)
y <- dat_scaled$cost
yhat <- matrix(0, n, n.models)
MSE <- matrix(0, Nrep, n.models)
model_num <- 1
for (j in 1:Nrep) {
  cv_idx <- createFolds(dat_scaled$cost, k = K)
  for (k in 1:K) {
    control_opti <- rpart.control(minbucket = 5, cp = 0.017)
    out_opti <- rpart(cost ~ ., data = dat_scaled, method = "anova", control = control_opti)
    yhat[cv_idx[[k]], model_num] <- as.numeric(predict(out_opti, dat_scaled[cv_idx[[k]], ]))
  } #end of k loop
  MSE[j, ] <- apply(yhat, 2, function(x)
    sum((y - x) ^ 2)) / n
}#end of j loop
    
MSEAve <- apply(MSE,2,mean)
MSEAve
```

As seen above, the best cp is 0.01.

### b

```{r q3b, echo=FALSE}
r2 <- 1 - MSEAve/var(y); r2 
```

As seen from the above plots and R^2, the decision tree algorithm explains `r r2 * 100` % of the variation in the data. It is still very good, but not as good as the neural network.

### c

```{r q3c, echo=FALSE}
pred <- predict(out_opti, dat_scaled)
res <- dat_scaled$cost - pred
plot(pred, res, main = "Fitted vs Residuals")
```

### d

As seen above, the neural network can explain the most amount of variance in this dataset, which means it has the least MSE. Thus, we choose neural network.

## Question 4

### a

```{r q4a, echo=FALSE}
########### QUESTION 4 begins here ################### 
dat <- fgl
set.seed(123)
Nrep <- 3 #number of replicates of CV
K <- 10  #K-fold CV on each replicate
n.models <- 8 #number of different models to fit
size <- c(5, 10)
decay <- c(0.001, 0.01, 0.1, 0)
n <- nrow(dat)
y <- dat$type
yhat <- matrix(0, n, n.models)
missclass <- matrix(0, Nrep, n.models)
output <- data.frame()
model_num <- 1
for (s in size) {
  for (d in decay) {
    for (j in 1:Nrep) {
      cv_idx <- createFolds(dat$type, k = K)
      for (k in 1:K) {
        out <-
          nnet(
            type ~ .,
            data = dat[-cv_idx[[k]],],
            linout = F,
            skip = F,
            size = s,
            decay = d,
            maxit = 1000,
            trace = F
          )
        yhat[cv_idx[[k]], model_num] <- predict(out, dat[cv_idx[[k]], ], type="class")
      } #end of k loop
      missclass[j, ] <- apply(yhat, 2, function(x) sum(y != x) / length(y))
    }#end of j loop
    output <- rbind(output, c(s, d))
    model_num <- model_num + 1
  }
}
```

```{r q4a_cont, echo=FALSE}
missclass_avg <- apply(missclass,2,mean)
output <- cbind(output, missclass_avg)
names(output) <- c("size", "decay", "Average CV Missclassification rate over 3 replicates")
output
```

### b

```{r q4b, echo=FALSE}
set.seed(123)
dat <- fgl
control <- rpart.control(minbucket = 5, maxdepth = 5)
out <- rpart(type ~ ., data = dat, method = "class", control = control)
plotcp(out)
printcp(out) 
#prune back to optimal size, according to plot of CV r^2
out <- prune(out, cp=0.022)  #approximately the cp corresponding to the best size
par(cex=.7); plot(out, uniform=F); text(out, use.n = F); par(cex=1)
out$variable.importance
out$cptable[nrow(out$cptable),]
out$cptable[nrow(out$cptable),][c(3,4)]*min(table(dat$type)/nrow(dat))  #training and cv misclass rates
yhat<-predict(out, type="class"); sum(yhat != dat$type)/nrow(dat) #check training misclass rate
```

### c

```{r q4c, echo=FALSE}
m2 <- multinom(type ~., data= dat, trace=FALSE)
yhat<-predict(m2, type="class")
sum(yhat != dat$type)/nrow(dat)
```

### d

```{r q4d, echo=FALSE}
dat <- fgl
set.seed(123)
Nrep <- 3 #number of replicates of CV
K <- 10 #K-fold CV on each replicate
n.models <-3 #number of different models to fit
size <- c(5, 10)
decay <- c(0.001, 0.01, 0.1, 0)
n <- nrow(dat)
y <- as.numeric(as.factor(dat$type))
yhat <- matrix(0, n, n.models)
missclass <- matrix(0, Nrep, n.models)
output <- data.frame()

for (j in 1:Nrep) {
  cv_idx <- createFolds(dat$type, k = K)
  for (k in 1:K) {
    out <-
      nnet(
        type ~ .,
        data = dat[-cv_idx[[k]], ],
        linout = F,
        skip = F,
        size = 10,
        decay = 0.01,
        maxit = 1000,
        trace = F,
        method = "class"
      )
    yhat[cv_idx[[k]], 1] <- as.numeric(factor(predict(out, dat[cv_idx[[k]], ], type="class"), levels = levels(dat$type)))
    
    control <- rpart.control(minbucket = 5, cp = 0.022)
    out <- rpart(type ~ ., data = dat[-cv_idx[[k]], ], method = "class", control = control)
    yhat[cv_idx[[k]], 2] <- predict(out, dat[cv_idx[[k]], ], type="class")
    
    out <- multinom(type ~., data = dat[-cv_idx[[k]], ], trace=FALSE)
    yhat[cv_idx[[k]], 3] <- predict(out, dat[cv_idx[[k]], ], type="class")
  } #end of k loop
  missclass[j, ] <- apply(yhat, 2, function(x) sum(y != x) / length(y))
}#end of j loop

missclass_avg <- apply(missclass, 2, mean)
missclass_avg
```

As seen above, the best model is neural network with the smallest missclassification error

# Appendix

```{r getlabels}
labs = knitr::all_labels()
labs = labs[!labs %in% c("setup","getlabels", "allcode")]
```

```{r allcode,ref.label=labs,eval=FALSE,echo=TRUE}
```